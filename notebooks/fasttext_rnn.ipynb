{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2eae04-b330-47df-8a69-6451e3b0a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514c4b3-f22d-4ce5-9662-68b3cec6eab1",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e3341a-ec17-47f4-823b-3cfe932ab76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes:  (1551, 60) (1551,)\n",
      "Test shapes:  (318, 60) (318,)\n"
     ]
    }
   ],
   "source": [
    "# import all data\n",
    "df = pd.read_csv('data/upload_DJIA_table.csv', parse_dates=['Date'], index_col='Date')\n",
    "df = df[['Close']]\n",
    "df = df.sort_index()\n",
    "\n",
    "train_data = df[:'2014']\n",
    "test_data = df['2015':]\n",
    "\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(df) - seq_length):\n",
    "      \t# Define inputs\n",
    "        x = df.iloc[i:i+seq_length, 0]\n",
    "        # Define target\n",
    "        y = df.iloc[i+seq_length, 0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, 60)\n",
    "X_test, y_test = create_sequences(test_data, 60)\n",
    "\n",
    "print(\"Train shapes: \", X_train.shape, y_train.shape)\n",
    "print(\"Test shapes: \", X_test.shape, y_test.shape)\n",
    "\n",
    "# convert to torch dataset\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float()\n",
    ")\n",
    "dataset_test = TensorDataset(\n",
    "    torch.from_numpy(X_test).float(),\n",
    "    torch.from_numpy(y_test).float()\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad469f-4ca3-494f-8df2-072f2c4ddc87",
   "metadata": {},
   "source": [
    "## Pure NLP \n",
    "\n",
    "Use embeddings for predicting price\n",
    "Vectorize data using:\n",
    "1. Word2Vec\n",
    "2. Tf-Idf\n",
    "3. Fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7afab8-c3a2-4865-9a2d-285dd8a99ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/baga_nuhkadiev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-08-08</th>\n",
       "      <td>georgia ' downs two russian warplanes ' countr...</td>\n",
       "      <td>11734.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-11</th>\n",
       "      <td>wont america nato help us ? wont help us , hel...</td>\n",
       "      <td>11782.349609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-12</th>\n",
       "      <td>remember adorable 9 - year - old sang opening ...</td>\n",
       "      <td>11642.469727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-13</th>\n",
       "      <td>u . . refuses israel weapons attack iran : rep...</td>\n",
       "      <td>11532.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-14</th>\n",
       "      <td>experts admit legalise drugs war south osetia ...</td>\n",
       "      <td>11615.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-27</th>\n",
       "      <td>barclays rbs shares suspended trading tanking ...</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-28</th>\n",
       "      <td>2 , 500 scientists australia : want save great...</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-29</th>\n",
       "      <td>explosion airport istanbul yemeni former presi...</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>jamaica proposes marijuana dispensers tourists...</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>117 - year - old woman mexico city finally rec...</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1989 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Text         Close\n",
       "Date                                                                       \n",
       "2008-08-08  georgia ' downs two russian warplanes ' countr...  11734.320312\n",
       "2008-08-11  wont america nato help us ? wont help us , hel...  11782.349609\n",
       "2008-08-12  remember adorable 9 - year - old sang opening ...  11642.469727\n",
       "2008-08-13  u . . refuses israel weapons attack iran : rep...  11532.959961\n",
       "2008-08-14  experts admit legalise drugs war south osetia ...  11615.929688\n",
       "...                                                       ...           ...\n",
       "2016-06-27  barclays rbs shares suspended trading tanking ...  17140.240234\n",
       "2016-06-28  2 , 500 scientists australia : want save great...  17409.720703\n",
       "2016-06-29  explosion airport istanbul yemeni former presi...  17694.679688\n",
       "2016-06-30  jamaica proposes marijuana dispensers tourists...  17929.990234\n",
       "2016-07-01  117 - year - old woman mexico city finally rec...  17949.369141\n",
       "\n",
       "[1989 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_news_djia = pd.read_csv('data/Combined_News_DJIA.csv')\n",
    "combined_news_djia['Top1'] = combined_news_djia['Top1'].apply(lambda x: x[2:-1] if x[0]=='b' else x)\n",
    "combined_news_djia['Top2'] = combined_news_djia['Top2'].apply(lambda x: x[2:-1] if x[0]=='b' else x)\n",
    "prices = pd.read_csv('data/upload_DJIA_table.csv')\n",
    "prices = prices[['Date', 'Close']]\n",
    "data = pd.DataFrame(columns=['Date', 'Text'])\n",
    "\n",
    "data['Text'] = combined_news_djia['Top1'] + \" \" + combined_news_djia['Top2']\n",
    "data['Date'] = combined_news_djia['Date']\n",
    "data['Close'] = prices.sort_values(by='Date').reset_index()['Close']\n",
    "data = data.set_index('Date')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_headline(x):\n",
    "    return \" \".join([w.lower() for w in tokenizer.tokenize(x) if not w.lower() in stop_words])\n",
    "\n",
    "data['Text'] = data['Text'].apply(process_headline)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d113413-9a7e-4984-b47e-899101e898e6",
   "metadata": {},
   "source": [
    "Tf-Idf return vectors size 11020. That's too much\n",
    "\n",
    "Let's try getting word2vec embeddings from fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7784695d-7e82-4f4d-95e7-f37e1dc4578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "\n",
    "# Download FastText word vectors\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # Download English language embeddings\n",
    "ft = fasttext.load_model('cc.en.300.bin')  # Load the downloaded model\n",
    "\n",
    "def get_embeddings(data):\n",
    "    combo = []\n",
    "    for row in data.values:\n",
    "        news_embedding = np.mean([ft.get_word_vector(word) for word in row[0].split()], axis=0)\n",
    "        combo.append(news_embedding)\n",
    "    return np.array(combo), data.values[:, 1]\n",
    "\n",
    "train_data = data[:'2014']\n",
    "test_data = data['2015':]\n",
    "train_emb = get_embeddings(train_data)\n",
    "test_emb = get_embeddings(train_data)\n",
    "\n",
    "# prices = data.values[:, 1].reshape(-1, 1)\n",
    "# d = np.hstack((combo, prices))\n",
    "# d[0, -1]\n",
    "\n",
    "X_train, y_train = get_embeddings(train_data)\n",
    "X_test, y_test = get_embeddings(test_data)\n",
    "\n",
    "# convert to torch dataset\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train.astype(float)).float()\n",
    ")\n",
    "dataset_test = TensorDataset(\n",
    "    torch.from_numpy(X_test).float(),\n",
    "    torch.from_numpy(y_test.astype(float)).float()\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13dc44d6-b02a-4a41-875e-0dda0cdb2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baga_nuhkadiev/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/baga_nuhkadiev/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 137767888.0, MAE: 11737.45662398801\n",
      "Epoch 2, Loss: 151986544.0, MAE: 12328.282280999247\n",
      "Epoch 3, Loss: 99173752.0, MAE: 9958.601909906833\n",
      "Epoch 4, Loss: 36034648.0, MAE: 6002.886638942968\n",
      "Epoch 5, Loss: 10408812.0, MAE: 3226.2690526364972\n",
      "Epoch 6, Loss: 9505107.0, MAE: 3083.035354970812\n",
      "Epoch 7, Loss: 12741957.0, MAE: 3569.587791328293\n",
      "Epoch 8, Loss: 8894268.0, MAE: 2982.3259379216083\n",
      "Epoch 9, Loss: 8255190.0, MAE: 2873.1846442580054\n",
      "Epoch 10, Loss: 8483179.0, MAE: 2912.5897411067012\n"
     ]
    }
   ],
   "source": [
    "predictor = nn.Sequential(\n",
    "    nn.Linear(300, 150),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for features, labels in dataloader_train:\n",
    "        outputs = predictor(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, MAE: {loss.item()**0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27b58cde-15b4-4091-b36c-ce668d3c9059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:  tensor(43151148.)\n",
      "Test MAE:  tensor(6568.9531)\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "mse = torchmetrics.MeanSquaredError()\n",
    "predictor.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_test:\n",
    "        outputs = predictor(features).squeeze()\n",
    "        mse(outputs, labels)\n",
    "\n",
    "print(\"Test MSE: \", mse.compute())\n",
    "print(\"Test MAE: \", mse.compute()**0.5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bceb5c-b7a7-4470-953c-77a1dbf3e155",
   "metadata": {},
   "source": [
    "## Modelling with BERT embeddings\n",
    "\n",
    "Here we build multi-input model that consists of\n",
    "\n",
    "RNN for stock prices in window of n\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f38c3c5-9543-4447-a0aa-793154aa6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/Users/baga_nuhkadiev/.cache/huggingface'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf38b987-51be-4a0a-8f26-a088b6b45854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e95986264e84d36a88021743780e5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadaf4753da2434790ce178f43e975b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbc993cb590495aaf3f3ee1a74a4f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baga_nuhkadiev/opt/anaconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13373f37207a440f883dae6aaa496cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93f9b7eb5b24836a9ee728480d314ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    # Get the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    # Pool the embeddings (use mean pooling for simplicity)\n",
    "    pooled_embeddings = torch.mean(embeddings, dim=1)\n",
    "    return pooled_embeddings.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a331aff-e95b-459c-a57c-04ac43bc5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = data['Text'].values\n",
    "embeddings = [get_bert_embeddings(headline) for headline in headlines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "734277af-4533-407e-a6e5-e6f6b4ea7ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 1, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings = np.array(embeddings)\n",
    "bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72bc25b0-5284-426c-af52-d05b40ad03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('bert_embeddings.npy', 'wb') as f:\n",
    "#     np.save(f, bert_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1dc7f2c-0188-49b8-9cc2-d4e8db302cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2008-08-08    11734.320312\n",
       "2008-08-11    11782.349609\n",
       "2008-08-12    11642.469727\n",
       "2008-08-13    11532.959961\n",
       "2008-08-14    11615.929688\n",
       "                  ...     \n",
       "2016-06-27    17140.240234\n",
       "2016-06-28    17409.720703\n",
       "2016-06-29    17694.679688\n",
       "2016-06-30    17929.990234\n",
       "2016-07-01    17949.369141\n",
       "Name: Close, Length: 1989, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b223989-6484-4326-8c47-3b7b7d74237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of stock prices and BERT embeddings\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "SEQ_LENGTH = 5\n",
    "\n",
    "# Extract stock prices and BERT embeddings\n",
    "stock_prices = df['Stock Price'].values\n",
    "bert_embeddings = np.stack(df['BERT Embedding'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
